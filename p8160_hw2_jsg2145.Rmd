---
title: "p8131_hw2_jsg2145"
author: "Jared Garfinkel"
date: "2/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(ResourceSelection)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Homework 2

This homework requires the use of logistic regression to practice fitting logistic models with given datasets.

## Problem 1

This problem explores the similarities and differences between link functions in linear regression. While the logit and probit link functions return similar results, it appears that the complementary log-log link function has a wider confidence interval and larger deviance, indicating a logistical regression model may be appropriate in this case.

## Problem 1i

Create a table of descriptive statistics for beta, a proxy for the effect size.

```{r}
# Create a dataframe

dos_df <- tibble(
  dose = c(0:4),
  dead = c(2, 8, 15, 23, 27),
  alive = 30 - dead
)
```

```{r}
# create a logistic regression

dos_logit <- dos_df %>% 
  glm(data = ., cbind(dead, alive) ~ dose, family = binomial(link = "logit"))

summary(dos_logit)
```

```{r}
# Create a normal regression

dos_probit <- dos_df %>% 
  glm(data = ., cbind(dead, alive) ~ dose, family = binomial(link = "probit"))

summary(dos_probit)
```

```{r}
# Create a complementary log-log regression
dos_cloglog <- dos_df %>% 
  glm(data = ., cbind(dead, alive) ~ dose, family = binomial(link = "cloglog"))

summary(dos_cloglog)
```
```{r}
# Return confidence intervals for each regression

dos_logit_ci = dos_logit %>% 
  broom::tidy() %>% 
  mutate(
    lower = (estimate - qnorm(.975) * std.error),
    upper = (estimate + qnorm(.975) * std.error)) %>%
  select(term, estimate, lower, upper) %>% 
  filter(term == "dose") %>% 
  mutate(term = recode(term, dose = "logit"))

dos_probit_ci = dos_probit %>% 
  broom::tidy() %>% 
  mutate(
    lower = (estimate - qnorm(.975) * std.error),
    upper = (estimate + qnorm(.975) * std.error)) %>% 
  select(term, estimate, lower, upper) %>% 
  filter(term == "dose") %>% 
  mutate(term = recode(term, dose = "probit"))

dos_cloglog_ci = dos_cloglog %>% 
  broom::tidy() %>% 
  mutate(
    lower = (estimate - qnorm(.975) * std.error),
    upper = (estimate + qnorm(.975) * std.error)) %>%
  select(term, estimate, lower, upper) %>% 
  filter(term == "dose") %>% 
  mutate(term = recode(term, dose = "cloglog"))

table = rbind(dos_logit_ci, dos_probit_ci, dos_cloglog_ci)
```

```{r}
# Return deviances for each regression

dev_logit = deviance(dos_logit)

dev_probit = deviance(dos_probit)

dev_cloglog = deviance(dos_cloglog)

deviance = c(dev_logit, dev_probit, dev_cloglog)

table = table %>% 
  mutate(deviance = c(dev_logit, dev_probit, dev_cloglog))
```

```{r}
# Return probability of event for dose = 0.01 on the dataframe

p_logit = predict(dos_logit, data.frame(dose=.01), se.fit=TRUE, type='response')

p_probit = predict(dos_probit, data.frame(dose=.01), se.fit=TRUE, type='response')

p_cloglog = predict(dos_cloglog, data.frame(dose=.01), se.fit=TRUE, type='response')

table = table %>% 
  mutate(p_estimate = c(round(p_logit[[1]], digits = 2), round(p_probit[[1]], digits = 2), round(p_cloglog[[1]], digits = 2))) %>% 

table %>%
  knitr::kable(digits = 2)

```

## Problem 1ii

The purpose of Problem 1ii is to use the variance-covariance matrix to calculate a 95% confidence interval of the effect size for the exposure in this dataset, dosage.

```{r}
# Return 95% confidence intervals for the betas and beta_0s for each regression

# logit

beta0_l=dos_logit$coefficients[1]
beta1_l=dos_logit$coefficients[2]
betacov_l=vcov(dos_logit)
x0fit_l=-beta0_l/beta1_l
est_l = exp(x0fit_l)

varx0_l=betacov_l[1,1]/(beta1_l^2)+betacov_l[2,2]*(beta0_l^2)/(beta1_l^4)-2*betacov_l[1,2]*beta0_l/(beta1_l^3)
est_sd_l = c(x0fit_l,sqrt(varx0_l))
ci_l = exp(x0fit_l+c(qnorm(0.05),-qnorm(0.05))*sqrt(varx0_l))

#probit

beta0_p=dos_probit$coefficients[1]
beta1_p=dos_probit$coefficients[2]
betacov_p=vcov(dos_probit)
x0fit_p=-beta0_p/beta1_p
est_p = exp(x0fit_p)

varx0_p=betacov_p[1,1]/(beta1_p^2)+betacov_p[2,2]*(beta0_p^2)/(beta1_p^4)-2*betacov_p[1,2]*beta0_p/(beta1_p^3)
est_sd_p = c(x0fit_p,sqrt(varx0_p)) # point est and se
ci_p = exp(x0fit_p+c(qnorm(0.05),-qnorm(0.05))*sqrt(varx0_p))

#cloglog

beta0_c=dos_cloglog$coefficients[1]
beta1_c=dos_cloglog$coefficients[2]
betacov_c=vcov(dos_cloglog)
x0fit_c=(log(log(2))-beta0_c)/beta1_c
est_c = exp(x0fit_c)

varx0_c=betacov_c[1,1]/(beta1_c^2)+betacov_c[2,2]*(beta0_c^2)/(beta1_c^4)-2*betacov_c[1,2]*beta0_c/(beta1_c^3)
est_sd_c = c(x0fit_c,sqrt(varx0_c)) # point est and se
ci_c = exp(x0fit_c+c(qnorm(0.05),-qnorm(0.05))*sqrt(varx0_c))

dos_ci = tibble(
  term = c("logit", "probit", "cloglog"),
  estimate = c(est_l, est_p, est_c),
  lower = c(ci_l[1], ci_p[1], ci_c[1]),
  upper = c(ci_l[2], ci_p[2], ci_c[2])
)

dos_ci %>% 
  knitr::kable(digits = 2)
```


## Problem 2

This problem focuses on how to use linear regression to determine the goodness of fit of a model with a dataset.

```{r}
# Create a dataframe

data = tibble(
  amount = seq(from = 10, to = 90, by = 5),
  offer = c(4,6,10,12,39,36,22,14,10,12,8,9,3,1,5,2,1),
  enrolls = c(0,2,4,2,12,14,10,7,5,5,3,5,2,0,4,2,1),
  passes = offer - enrolls
)
```

## Problem 2i

This question uses our knowledge of logistic regression to study the goodness of fit of the given data. A Hosmer-Lemeshow test is used for a dataset with small sample size. The Hosmer-Lemeshow and Pearson Chi-Square Residual tests show the model is a good fit.

```{r}
# Fit a logistic regression

dat_glm = data %>% glm(data = ., cbind(enrolls, passes) ~ amount, family = binomial(link = 'logit'))
summary(dat_glm)

# Run diagnostics for small sample sizes (Hos-Lem)

dev_glm = deviance(dat_glm)
hl = hoslem.test(dat_glm$y, fitted(dat_glm), g = 10)

hl

pval = 1-pchisq(dev_glm,15)

pval
```

The p-values are greater than 0.05, indicating that the model is a good fit.

## Problem 2ii

```{r}
# Return 95% confidence interval for the estimate of beta

dat_tbl = dat_glm %>%
  broom::tidy() %>% 
  mutate(
    lower = estimate - qnorm(.975) * std.error,
    upper = estimate + qnorm(.975) * std.error,
  ) %>% 
  filter(term == "amount") %>% 
  select(term, estimate, lower, upper)

dat_tbl %>% 
  knitr::kable(digits = 2)
  
```

The odds ratio of enrolling in this MPH program increases by `r round((exp(pull(dat_tbl, estimate))-1)*100, digits = 2)`, CI: (`r round((exp(pull(dat_tbl, lower))-1)*100, digits = 2)`, `r round((exp(pull(dat_tbl, upper))-1)*100, digits = 2)`) percent per $1000 offered.

## Problem 2iii

```{r}
beta0_mph=dat_glm$coefficients[1]
beta1_mph=dat_glm$coefficients[2]
betacov_mph=vcov(dat_glm)
x0_mph=(log(0.4/0.6) - beta0_mph) / beta1_mph
est_mph = x0_mph*1000

varx0_mph=betacov_mph[1,1]/(beta1_mph^2)+betacov_mph[2,2]*(beta0_mph^2)/(beta1_mph^4)-2*betacov_mph[1,2]*beta0_mph/(beta1_mph^3)
est_sd_mph = c(x0_mph,sqrt(varx0_mph))
ci_mph = (x0_mph+c(qnorm(0.025),-qnorm(0.025))*sqrt(varx0_mph))*1000

ci_tbl = tibble(
  term = "logit",
  estimate = est_mph,
  lower = ci_mph[1],
  upper = ci_mph[2]
)

ci_tbl %>% 
  knitr::kable(digits = 0)
```

On average, an offer of `r format(round(pull(ci_tbl, estimate), digits = 0), scientific = FALSE)` dollars would result in a 40% yield rate. With 95% confidence, an offer of between `r format(round(pull(ci_tbl, lower), digits = 0), scientific = FALSE)` dollars and `r format(round(pull(ci_tbl, upper), digits = 0), scientific = FALSE)` dollars would result in a 40% yield rate.